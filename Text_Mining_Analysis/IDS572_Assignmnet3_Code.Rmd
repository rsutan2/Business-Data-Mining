---
title: "Assignment3_Yelp"
output: html_document
date: "2022-11-18"
---

```{r}
library(tidyverse)
library(readr)
library(tidytext)
library(SnowballC)
library(textstem)
library(textdata)
library(caret)
library(rsample)
library(ranger)
library(dplyr)
library(pROC)
library(e1071)
library(ROCR)
```

```{r}
resReviewsData <- read_csv2('yelpRestaurantReviews_sample_f22.csv')
glimpse(resReviewsData)
```

```{r}
ggplot(resReviewsData, aes(x= starsBusiness, y=starsReview)) +geom_point()
ggplot(resReviewsData, aes(x= starsReview, y=starsBusiness)) +geom_point()
resReviewsData %>% group_by(starsReview) %>% count()
resReviewsData %>% group_by(starsBusiness) %>% count()
hist(resReviewsData$starsReview)
hist(resReviewsData$starsBusiness)
resReviewsData %>% group_by(state) %>% count()
ggplot(data.frame(resReviewsData), aes(x=state)) + geom_bar()
```

```{r}
rrTokens <- resReviewsData %>% select(review_id, starsReview, text ) %>% unnest_tokens(word, text)
dim(rrTokens)
head(rrTokens)
#count the total occurrences of different words, & sort by most frequent
rrTokens %>% count(word, sort=TRUE) %>% top_n(10)
#How many distinct terms?
rrTokens %>% distinct(word) %>% dim()
#remove stopwords
rrTokens <- rrTokens %>% anti_join(stop_words)
#Are there some rare terms, which occur in very few reviews?
#Let's remove the words which are not present in at least 10(??) reviews
rareWords <-rrTokens %>% count(word, sort=TRUE) %>% filter(n<10)
rareWords
#Remove these
xx <- anti_join (rrTokens, rareWords)
#any remaining words to remove -- check the words in xx
xx %>% count(word, sort=TRUE) %>% view()
#Remove the terms containing digits?
xx <- xx %>% filter(str_detect(word,"[0-9]") == FALSE)
#commit if you want these changes
rrTokens<- xx
#How many distinct tokens remain ?
rrTokens %>% distinct(word) %>% dim()
#Check words by star rating of reviews
rrTokens %>% group_by(starsReview) %>% count(word, sort=TRUE)
#proportion of word occurrence by star ratings
ws <- rrTokens %>% group_by(starsReview) %>% count(word, sort=TRUE)
ws<- ws %>% group_by(starsReview) %>% mutate(prop=n/sum(n))
#check the proportion of 'love' among reviews with 1,2,..5 starsReview 
ws %>% filter(word=='love')
#what are the most commonly used words by star rating
ws %>% group_by(starsReview) %>% arrange(starsReview, desc(prop)) %>% view()
#to see the top 20 words by star ratings
ws %>% group_by(starsReview) %>% arrange(starsReview, desc(prop))%>% filter(row_number()<=20) %>% view()
#To plot this
ws %>% group_by(starsReview) %>% arrange(starsReview, desc(prop))%>% filter(row_number()<=20)%>% ggplot(aes(word,prop))+geom_col()+coord_flip()+facet_wrap((~starsReview))
# plot without words like ‘food’, ‘time’,… which occurs across ratings
ws %>% filter(! word %in% c('food', 'time', 'restaurant', 'service')) %>% group_by(starsReview) %>% arrange(starsReview,desc(prop))%>%filter(row_number() <=15) %>%ggplot(aes(word,prop))+geom_col()+coord_flip()+facet_wrap((~starsReview))
xx<- ws %>% group_by(word) %>% summarise( totWS = sum(starsReview*prop))
rrTokens <- rrTokens_wo_cw
ws_afterprune <- rrTokens %>% group_by(starsReview) %>% count(word, sort=TRUE)
ws_afterprune<- ws_afterprune %>% group_by(starsReview) %>% mutate(prop=n/sum(n))
ws_afterprune %>% group_by(starsReview) %>% arrange(starsReview, desc(prop)) %>% filter(row_number()<=20) %>% view()
ws_afterprune %>% group_by(starsReview) %>% arrange(starsReview, desc(prop)) %>% filter(row_number()<=10) %>% ggplot(aes(word, prop))+geom_col()+coord_flip()+facet_wrap((~starsReview))
#What are the 20 words with highest and lowest star rating
xx %>% top_n(20)
xx %>% top_n(-20)
#Stemming and Lemmatization
rrTokens_stem <- rrTokens %>% mutate(word_stem = SnowballC::wordStem(word))
rrTokens_lemm <- rrTokens %>% mutate(word_lemma = textstem::lemmatize_words(word))
#tokenize, remove stopwords, and lemmatize
rrTokens<-rrTokens %>% mutate(word = textstem::lemmatize_words(word))
#filter out words with less than 3 characters more than 15 characters (??)
rrTokens<-rrTokens %>% filter(str_length(word)<=3 | str_length(word)<=15)
rrTokens<- rrTokens %>% group_by(review_id, starsReview) %>% count(word)
#count total number of words by review, and add this in a column
totWords <- rrTokens %>% group_by(review_id)%>% count(word, sort=TRUE) %>% summarise(total=sum(n))
#add the column of counts
xx<-left_join(rrTokens, totWords)
# now n/total gives the tf values
xx<-xx %>% mutate(tf=n/total)
head(xx)
#Or, can tokenize, remove stopwords, lemmatize i as
rrTokens <- resReviewsData %>% select(review_id, starsReview, text ) %>% unnest_tokens(word, text) %>% anti_join(stop_words)%>% mutate(word = textstem::lemmatize_words(word))
#We can use the bind_tfidf function to calculate the tf, idf and tfidf values
# (https://www.rdocumentation.org/packages/tidytext/versions/0.2.2/topics/bind_tf_idf)
xx_afterprune<- ws_afterprune %>% group_by(word) %>% summarise(totWS=sum(starsReview*prop))
xx_afterprune
xx_afterprune %>% top_n(20)
xx_afterprune %>% top_n(-20)

rrTokens_stem<-rrTokens %>% mutate(word_stem = SnowballC::wordStem(word))
rrTokens_lemm<-rrTokens %>% mutate(word_lemma = textstem::lemmatize_words(word))
rrTokens <- rrTokens %>% mutate(word = textstem::lemmatize_words(word))
rrTokens <- rrTokens %>% filter(str_length(word)<=3 | str_length(word)<=15)
rrTokens<- rrTokens %>% group_by(review_id, starsReview) %>% count(word)
totWords<-rrTokens %>% group_by(review_id) %>% count(word, sort=TRUE) %>% summarise(total=sum(n))
rrTokens_totwords<-left_join(rrTokens, totWords)

rrTokens_totwords<-rrTokens_totwords %>% mutate(tf=n/total)
head(rrTokens_totwords)
rrTokens<-rrTokens %>% bind_tf_idf(word, review_id, n)
```

```{r}
#take a look at the words in the sentiment dictionaries
bing <- get_sentiments("bing")
dim(bing)
nrc <- get_sentiments("nrc")
dim(nrc)
afinn <- get_sentiments("afinn")
dim(afinn)
#count number of mathing words
dim(matching_words <- bing %>% inner_join(nrc, by = "word") %>% inner_join(afinn, by = "word"))
```

```{r}
#get sentiment of words in rrTokens – using join
rrSenti_bing<- rrTokens %>% left_join( get_sentiments("bing"), by="word")
#to retain only the words which match the sentiment dictionary, do an inner-join
rrSenti_bing<- rrTokens %>% inner_join( get_sentiments("bing"), by="word")
#count the occurrences of positive/negative sentiment words in the reviews
xx <- rrSenti_bing %>% group_by(word, sentiment) %>% summarise(totOcc=sum(n)) %>% arrange(sentiment, desc(totOcc))
xx
#negate the counts for the negative sentiment words
xx<- xx %>% mutate (totOcc=ifelse(sentiment=="positive", totOcc, -totOcc))
xx
# which are the most positive and most negative words in reviews
xx<-ungroup(xx)
xx %>% top_n(25)
xx %>% top_n(-25)
#You can plot these
rbind(top_n(xx, 25), top_n(xx, -25)) %>% ggplot(aes(word, totOcc, fill=sentiment)) +geom_col()+coord_flip()
#or, with a better reordering of words
rbind(top_n(xx, 25), top_n(xx, -25)) %>% mutate(word=reorder(word,totOcc))%>% ggplot(aes(word, totOcc, fill=sentiment)) +geom_col()+coord_flip()
#with "nrc" dictionary
rrSenti_nrc<-rrTokens %>% inner_join(get_sentiments("nrc"), by="word") %>%
group_by (word, sentiment) %>% summarise(totOcc=sum(n)) %>%
arrange(sentiment, desc(totOcc))
#How many words are there for the different sentiment categories
rrSenti_nrc %>% group_by(sentiment) %>% summarise(count=n(), sumn=sum(totOcc))
#top few words for different sentiments
rrSenti_nrc %>% group_by(sentiment) %>% arrange(sentiment, desc(totOcc))%>% top_n(10) %>% view()
xx<-rrSenti_nrc %>% mutate(goodBad=ifelse(sentiment %in% c('anger', 'disgust', 'fear', 'sadness', 'negative'), -totOcc,
ifelse(sentiment %in% c('positive', 'joy', 'anticipation', 'trust'), totOcc, 0)))
xx<-ungroup(xx)
top_n(xx, -20)
top_n(xx, 20)
```

```{r}
#with "bing" dictionary
rrSenti_bing<- rrTokens%>% inner_join(get_sentiments("bing"), by="word")
#summarise positive/negative sentiment words per review
revSenti_bing <- rrSenti_bing %>% group_by(review_id, starsReview)%>%summarise(nwords=n(),posSum=sum(sentiment=='positive'),negSum=sum(sentiment=='negative'))
#calculate sentiment score based on proportion of positive, negative words
revSenti_bing<- revSenti_bing %>% mutate(posProp=posSum/nwords, negProp=negSum/nwords)
revSenti_bing<- revSenti_bing%>% mutate(sentiScore=posProp-negProp)
#Do review star ratings correspond to the positive/negative sentiment words
revSenti_bing %>% group_by(starsReview) %>%
summarise( avgPos=mean(posProp), avgNeg=mean(negProp), avgSentiSc=mean(sentiScore))

#considering reviews with 1 to 2 stars as negative, and this with 4 to 5 stars as positive
revSenti_bing <- revSenti_bing %>% mutate(hiLo=ifelse(starsReview<=2,-1, ifelse(starsReview>=4, 1, 0 )))
revSenti_bing <- revSenti_bing %>% mutate(pred_hiLo=ifelse(sentiScore > 0.2075275, 1, -1))
#filter out the reviews with 3 stars, and get the confusion matrix for hiLo vs pred_hiLo
xx<-revSenti_bing %>% filter(hiLo!=0)
#table(actual=bing_predict$hiLo, predicted=bing_predict$pred_hiLo )

#Creates vectors having data points
expected_value <- factor(c(xx$hiLo))
predicted_value <- factor(c(xx$pred_hiLo))
accuracy<- confusionMatrix(data=predicted_value, reference = expected_value)
accuracy
```

```{r}
#with "afinn" dictionary
rrSenti_afinn<- rrTokens %>% inner_join(get_sentiments("afinn"), by="word")
revSenti_afinn <- rrSenti_afinn %>% group_by(review_id, starsReview)%>% summarise(nwords=n(), sentiSum =sum(value))
revSenti_afinn %>% group_by(starsReview)%>% summarise(avgLen=mean(nwords), avgSenti=mean(sentiSum))
#considering reviews with 1 to 2 stars as negative, and reviews with 4 to 5 stars as positive
revSenti_afinn <- revSenti_afinn %>% mutate ( hiLo = ifelse(starsReview <= 2, -1, ifelse(starsReview >=4, 1, 0 )))
revSenti_afinn <- revSenti_afinn %>% mutate(pred_hiLo = ifelse(sentiSum > 0, 1, -1) )
#filter out the reviews with 3 stars (ie hiLo=0), and get the confusion matrix for hiLo vs pred_hiLo
xx<-revSenti_afinn %>% filter(hiLo!=0)
#table(actual=xx$hiLo, predicted=xx$pred_hiLo )

#Creates vectors having data points
expected_value <- factor(c(xx$hiLo))
predicted_value <- factor(c(xx$pred_hiLo))
accuracy<- confusionMatrix(data=predicted_value, reference = expected_value)
accuracy
```

```{r}
#with "nrc" dictionary
#aggregate Positive/Negative score for each review using nrc
rrSenti_nrc<- rrTokens %>% inner_join(get_sentiments("nrc"), by="word")
#summarise positive/negative sentiment words per review
revSenti_nrc <- rrSenti_nrc %>% group_by(review_id, starsReview) %>% summarise(nwords=n(),posSum=sum(sentiment=='positive'),negSum=sum(sentiment=='negative'))
#calculate sentiment score based on proportion of positive, negative words
revSenti_nrc<- revSenti_nrc %>% mutate(posProp=posSum/nwords, negProp=negSum/nwords) 
revSenti_nrc<- revSenti_nrc %>% mutate(sentiScore=posProp-negProp) #view(revSenti_nrc)
#Do review star ratings correspond to the positive/negative sentiment words 
revSenti_nrc %>% group_by(starsReview) %>%summarise(avgPos=mean(posProp), avgNeg=mean(negProp), avgSentiSc=mean(sentiScore))

#considering reviews with 1 to 2 stars as negative, and this with 4 to 5 stars as positive
revSenti_nrc <- revSenti_nrc %>% mutate(hiLo=ifelse(starsReview<=2,-1, ifelse(starsReview>=4, 1, 0 )))
revSenti_nrc <- revSenti_nrc %>% mutate(pred_hiLo=ifelse(sentiScore > 0.140738725, 1, -1))
#filter out the reviews with 3 stars, and get the confusion matrix for hiLo vs pred_hiLo
xx<-revSenti_nrc %>% filter(hiLo!=0)
#table(actual=nrc_predict$hiLo, predicted=nrc_predict$pred_hiLo )

#Creates vectors having data points
expected_value <- factor(c(xx$hiLo))
predicted_value <- factor(c(xx$pred_hiLo))
accuracy<- confusionMatrix(data=predicted_value, reference = expected_value)
accuracy
```

```{r Bing}
#use pivot_wider to convert to a dtm form where each row is for a review and columns correspond to words
revDTM_sentiBing <- rrSenti_bing %>% pivot_wider(id_cols = review_id, names_from = word, values_from = tf_idf)
#Or, since we want to keep the stars column
revDTM_sentiBing <- rrSenti_bing %>% pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf) %>% ungroup()
#filter out the reviews with stars=3, and calculate hiLo sentiment 'class'
revDTM_sentiBing <- revDTM_sentiBing %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)
revDTM_sentiBing %>% group_by(hiLo) %>% tally()
#replace all the NAs with 0
revDTM_sentiBing <- revDTM_sentiBing %>% replace(., is.na(.), 0)
revDTM_sentiBing$hiLo <- as.factor(revDTM_sentiBing$hiLo)
#split the data into trn, tst subsets
set.seed(123)
nr=nrow(revDTM_sentiBing)
trnIndex = sample(1:nr, size = round(0.5*nr), replace=FALSE)
revDTM_sentiBing_SubSample=revDTM_sentiBing[trnIndex,]
revDTM_sentiBing_split<- initial_split(revDTM_sentiBing_SubSample, 0.7)
revDTM_sentiBing_trn<- training(revDTM_sentiBing_split)
revDTM_sentiBing_inter<- testing(revDTM_sentiBing_split)
revDTM_sentiBing_split_1<- initial_split(revDTM_sentiBing_inter, 0.66)
revDTM_sentiBing_tst<- training(revDTM_sentiBing_split_1)
revDTM_sentiBing_valid<- testing(revDTM_sentiBing_split_1)
dim(revDTM_sentiBing_trn)
dim(revDTM_sentiBing_tst)
dim(revDTM_sentiBing_valid)
colMeans(is.na(revDTM_sentiBing_trn))[colMeans(is.na(revDTM_sentiBing_trn))>0]
rm(revDTM_sentiBing_SubSample)
rm(revDTM_sentiBing_inter)
rm(revDTM_sentiBing_split)
rm(revDTM_sentiBing_split_1)
```

```{r NRC}
#remove duplicates from rrSenti_nrc, we only need the tf_idf score
rrSenti_nrc <-rrSenti_nrc[,-8]
rrSenti_nrc <-rrSenti_nrc[!duplicated(rrSenti_nrc), ]
revDTM_sentinrc <- rrSenti_nrc %>% pivot_wider(id_cols = c(review_id,starsReview), names_from = word,
 values_from = tf_idf) %>% ungroup()
#filter out the reviews with stars=3, and calculate hiLo sentiment 'class'
revDTM_sentinrc <- revDTM_sentinrc %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1))%>% select(-starsReview)
revDTM_sentinrc %>% group_by(hiLo) %>% tally()
#replace all the NAs with 0
revDTM_sentinrc <- revDTM_sentinrc %>% replace(., is.na(.), 0)
revDTM_sentinrc$hiLo <- as.factor(revDTM_sentinrc$hiLo)
#split the data into trn, tst subsets
set.seed(123)
nr=nrow(revDTM_sentinrc)
trnIndex = sample(1:nr, size = round(0.4*nr), replace=FALSE)
revDTM_sentinrc_SubSample=revDTM_sentinrc[trnIndex,]
revDTM_sentinrc_split<- initial_split(revDTM_sentinrc_SubSample, 0.7) 
revDTM_sentinrc_trn<- training(revDTM_sentinrc_split) 
revDTM_sentinrc_inter<- testing(revDTM_sentinrc_split)
revDTM_sentinrc_split_1<- initial_split(revDTM_sentinrc_inter, 0.66)
revDTM_sentinrc_tst<- training(revDTM_sentinrc_split_1)
revDTM_sentinrc_valid<- testing(revDTM_sentinrc_split_1)
#replace all the NAs with 0
revDTM_sentinrc_trn <- revDTM_sentinrc_trn %>% replace(., is.null(.), 0)
revDTM_sentinrc_trn$hiLo <- as.factor(revDTM_sentinrc_trn$hiLo)
dim(revDTM_sentinrc_trn)
dim(revDTM_sentinrc_tst)
dim(revDTM_sentinrc_valid)
rm(revDTM_sentinrc_SubSample)
rm(revDTM_sentinrc_inter)
rm(revDTM_sentinrc_split)
rm(revDTM_sentinrc_split_1)
```

```{r AFINN}
revDTM_sentiafinn <- rrSenti_afinn %>% pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf) %>% ungroup()
#filter out the reviews with stars=3, and calculate hiLo sentiment 'class'
revDTM_sentiafinn <- revDTM_sentiafinn %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)
revDTM_sentiafinn %>% group_by(hiLo) %>% tally()
#replace all the NAs with 0
revDTM_sentiafinn <- revDTM_sentiafinn %>% replace(., is.na(.), 0)
revDTM_sentiafinn$hiLo <- as.factor(revDTM_sentiafinn$hiLo)
#split the data into trn, tst subsets
set.seed(123)
nr=nrow(revDTM_sentiafinn)
trnIndex = sample(1:nr, size = round(0.5*nr), replace=FALSE)
revDTM_sentiafinn_SubSample=revDTM_sentiafinn[trnIndex,]
revDTM_sentiafinn_split<- initial_split(revDTM_sentiafinn_SubSample, 0.7) 
revDTM_sentiafinn_trn<- training(revDTM_sentiafinn_split) 
revDTM_sentiafinn_inter<- testing(revDTM_sentiafinn_split)
revDTM_sentiafinn_split_1<- initial_split(revDTM_sentiafinn_inter, 0.66)
revDTM_sentiafinn_tst<- training(revDTM_sentiafinn_split_1)
revDTM_sentiafinn_valid<- testing(revDTM_sentiafinn_split_1)
dim(revDTM_sentiafinn_trn)
dim(revDTM_sentiafinn_tst)
dim(revDTM_sentiafinn_valid)
rm(revDTM_sentiafinn_SubSample)
rm(revDTM_sentiafinn_inter)
rm(revDTM_sentiafinn_split)
rm(revDTM_sentiafinn_split_1)
```

```{r bing}
rfModel1_bing<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiBing_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)
#Obtain predictions, and calculate performance
revSentiBing_predTrn<- predict(rfModel1_bing, revDTM_sentiBing_trn %>% select(-review_id))$predictions
revSentiBing_predValid<- predict(rfModel1_bing, revDTM_sentiBing_valid %>% select(-review_id))$predictions
revSentiBing_predTst<- predict(rfModel1_bing, revDTM_sentiBing_tst %>% select(-review_id))$predictions
#Confusion matrix
table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn[,2]>0.5)
#Obtain predictions, and calculate performance
revSentiBing_predTrn<- predict(rfModel1_bing, revDTM_sentiBing_trn %>% select(-review_id))$predictions
revSentiBing_predValid<- predict(rfModel1_bing, revDTM_sentiBing_valid %>% select(-review_id))$predictions
revSentiBing_predTst<- predict(rfModel1_bing, revDTM_sentiBing_tst %>% select(-review_id))$predictions
#Confusion matrix
table(actual=revDTM_sentiBing_trn$hiLo, preds=revSentiBing_predTrn[,2]>0.5)
table(actual=revDTM_sentiBing_valid$hiLo, preds=revSentiBing_predValid[,2]>0.5)
table(actual=revDTM_sentiBing_tst$hiLo, preds=revSentiBing_predTst[,2]>0.5)
#ROC AUC graph
rocTrn_RFbing <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_predTrn[,2], levels=c(-1, 1))
rocTst_RFbing <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_predTst[,2], levels=c(-1, 1))
plot.roc(rocTrn_RFbing, col='blue')
plot.roc(rocTst_RFbing, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r nrc}
rfModel1_nrc<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentinrc_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)
#Obtain predictions, and calculate performance
revSentinrc_predTrn<- predict(rfModel1_nrc, revDTM_sentinrc_trn %>% select(-review_id))$predictions
revSentinrc_predValid<- predict(rfModel1_nrc, revDTM_sentinrc_valid %>% select(-review_id))$predictions
revSentinrc_predTst<- predict(rfModel1_nrc, revDTM_sentinrc_tst %>% select(-review_id))$predictions
#Confusion matrix
table(actual=revDTM_sentinrc_trn$hiLo, preds=revSentinrc_predTrn[,2]>0.5)
table(actual=revDTM_sentinrc_valid$hiLo, preds=revSentinrc_predValid[,2]>0.5)
table(actual=revDTM_sentinrc_tst$hiLo, preds=revSentinrc_predTst[,2]>0.5)
#ROC AUC graph
rocTrn_RFnrc <- roc(revDTM_sentinrc_trn$hiLo, revSentinrc_predTrn[,2], levels=c(-1, 1))
rocTst_RFnrc <- roc(revDTM_sentinrc_tst$hiLo, revSentinrc_predTst[,2], levels=c(-1, 1))
plot.roc(rocTrn_RFnrc, col='blue')
plot.roc(rocTst_RFnrc, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r afinn}
rfModel1_afinn<-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiafinn_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)
#Obtain predictions, and calculate performance
revSentiafinn_predTrn<- predict(rfModel1_afinn, revDTM_sentiafinn_trn %>% select(-review_id))$predictions
revSentiafinn_predValid<- predict(rfModel1_afinn, revDTM_sentiafinn_valid %>% select(-review_id))$predictions
revSentiafinn_predTst<- predict(rfModel1_afinn, revDTM_sentiafinn_tst %>% select(-review_id))$predictions
#Confusion matrix
table(actual=revDTM_sentiafinn_trn$hiLo, preds=revSentiafinn_predTrn[,2]>0.5)
table(actual=revDTM_sentiafinn_valid$hiLo, preds=revSentiafinn_predValid[,2]>0.5)
table(actual=revDTM_sentiafinn_tst$hiLo, preds=revSentiafinn_predTst[,2]>0.5)
#ROC AUC graph
rocTrn_RFafinn <- roc(revDTM_sentiafinn_trn$hiLo, revSentiafinn_predTrn[,2], levels=c(-1, 1))
rocTst_RFafinn <- roc(revDTM_sentiafinn_tst$hiLo, revSentiafinn_predTst[,2], levels=c(-1, 1))
plot.roc(rocTrn_RFafinn, col='blue')
plot.roc(rocTst_RFafinn, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r bing}
nbModel1<-naiveBayes(hiLo ~ ., data=revDTM_sentiBing_trn %>% select(-review_id))
revSentiBing_NBpredTrn<-predict(nbModel1, revDTM_sentiBing_trn, type = "raw")
revSentiBing_NBpredTst<-predict(nbModel1, revDTM_sentiBing_tst, type = "raw")
revSentiBing_NBpredValid<-predict(nbModel1, revDTM_sentiBing_valid, type = "raw")
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revSentiBing_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revSentiBing_NBpredTst[,2]>0.5)
table(actual= revDTM_sentiBing_valid$hiLo, predicted= revSentiBing_NBpredValid[,2]>0.5)
auc(as.numeric(revDTM_sentiBing_trn$hiLo), revSentiBing_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiBing_tst$hiLo), revSentiBing_NBpredTst[,2])
auc(as.numeric(revDTM_sentiBing_valid$hiLo), revSentiBing_NBpredValid[,2])
#ROC AUC graph
rocTrn_NBbing <- roc(revDTM_sentiBing_trn$hiLo, revSentiBing_NBpredTrn[,2], levels=c(-1, 1))
rocTst_NBbing <- roc(revDTM_sentiBing_tst$hiLo, revSentiBing_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn_NBbing, col= 'blue')
plot.roc(rocTst_NBbing, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r nrc}
nbModel2<-naiveBayes(hiLo ~ ., data=revDTM_sentinrc_trn %>% select(-review_id))
revSentinrc_NBpredTrn<-predict(nbModel2, revDTM_sentinrc_trn, type = "raw")
revSentinrc_NBpredTst<-predict(nbModel2, revDTM_sentinrc_tst, type = "raw")
revSentinrc_NBpredValid<-predict(nbModel2, revDTM_sentinrc_valid, type = "raw")
table(actual= revDTM_sentinrc_trn$hiLo, predicted= revSentinrc_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentinrc_tst$hiLo, predicted= revSentinrc_NBpredTst[,2]>0.5)
table(actual= revDTM_sentinrc_valid$hiLo, predicted= revSentinrc_NBpredValid[,2]>0.5)
auc(as.numeric(revDTM_sentinrc_trn$hiLo), revSentinrc_NBpredTrn[,2])
auc(as.numeric(revDTM_sentinrc_tst$hiLo), revSentinrc_NBpredTst[,2])
auc(as.numeric(revDTM_sentinrc_valid$hiLo), revSentinrc_NBpredValid[,2])
#ROC AUC graph
rocTrn <- roc(revDTM_sentinrc_trn$hiLo, revSentinrc_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentinrc_tst$hiLo, revSentinrc_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col= 'blue')
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r afinn}
nbModel3<-naiveBayes(hiLo ~ ., data=revDTM_sentiafinn_trn %>% select(-review_id))
revSentiafinn_NBpredTrn<-predict(nbModel3, revDTM_sentiafinn_trn, type = "raw")
revSentiafinn_NBpredTst<-predict(nbModel3, revDTM_sentiafinn_tst, type = "raw")
revSentiafinn_NBpredValid<-predict(nbModel3, revDTM_sentiafinn_valid, type = "raw")
table(actual= revDTM_sentiafinn_trn$hiLo, predicted= revSentiafinn_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiafinn_tst$hiLo, predicted= revSentiafinn_NBpredTst[,2]>0.5)
table(actual= revDTM_sentiafinn_valid$hiLo, predicted= revSentiafinn_NBpredValid[,2]>0.5)
auc(as.numeric(revDTM_sentiafinn_trn$hiLo), revSentiafinn_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiafinn_tst$hiLo), revSentiafinn_NBpredTst[,2])
auc(as.numeric(revDTM_sentiafinn_valid$hiLo), revSentiafinn_NBpredValid[,2])
rocTrn <- roc(revDTM_sentiafinn_trn$hiLo, revSentiafinn_NBpredTrn[,2], levels=c(-1, 1))
rocTst <- roc(revDTM_sentiafinn_tst$hiLo, revSentiafinn_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn, col= 'blue')
plot.roc(rocTst, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r bing}
#develop a SVM model on the sentiment dictionary terms
svmM1_bing <- svm(as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>% select(-review_id), kernel ="radial", cost=1, scale=FALSE)
#scale is set to TRUE by default. Since all vars are in tfidf, we shud set scale=FALSE revDTM_predTrn_svm1_bing<-predict(svmM1_bing, revDTM_sentiBing_trn) revDTM_predValid_svm1_bing<-predict(svmM1_bing, revDTM_sentiBing_valid) revDTM_predTst_svm1_bing<-predict(svmM1_bing, revDTM_sentiBing_tst)
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revDTM_predTrn_svm1_bing)
table(actual= revDTM_sentiBing_valid$hiLo, predicted= revDTM_predValid_svm1_bing)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revDTM_predTst_svm1_bing)
# try different parameters -- rbf kernel gamma, and cost
system.time( svmM2_bing <- svm(as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE))
revDTM_predTrn_svm2_bing<-predict(svmM2_bing, revDTM_sentiBing_trn)
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revDTM_predTrn_svm2_bing)
revDTM_predValid_svm2_bing<-predict(svmM2_bing, revDTM_sentiBing_valid)
table(actual= revDTM_sentiBing_valid$hiLo, predicted= revDTM_predValid_svm2_bing)
revDTM_predTst_svm2_bing<-predict(svmM2_bing, revDTM_sentiBing_tst)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revDTM_predTst_svm2_bing)
#use the tune function to do a grid search over a set of parameter values
#system.time(svm_tune <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>% select(-review_id),
#kernel="radial", ranges = list( cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10))) )
#Check performance for different tuned parameters
#svm_tune$performances
#Best model
#svm_tune$best.parameters
#svm_tune$best.model
system.time( svm_bing_best <- svm(as.factor(hiLo) ~., data = revDTM_sentiBing_trn %>% select(-review_id), kernel="radial", cost=10, gamma=0.5, scale=FALSE,decision.values=TRUE) )
#predictions from best model
revBing_predTrn_svm_best<-predict(svm_bing_best, revDTM_sentiBing_trn,decision.values=TRUE)
table(actual= revDTM_sentiBing_trn$hiLo, predicted= revBing_predTrn_svm_best)
revBing_predValid_svm_best<-predict(svm_bing_best, revDTM_sentiBing_valid,decision.values=TRUE)
table(actual= revDTM_sentiBing_valid$hiLo, predicted= revBing_predValid_svm_best)
revBing_predTst_svm_best<-predict(svm_bing_best, revDTM_sentiBing_tst,decision.values=TRUE)
table(actual= revDTM_sentiBing_tst$hiLo, predicted= revBing_predTst_svm_best)
#ROC graph SVM COMBINED best
rocTrn_svmBing<-prediction(attributes(revBing_predTrn_svm_best)$decision.values,revDTM_sentiBing_trn$hiLo)
rocTrn_svmBing_auc<-performance(rocTrn_svmBing,'tpr','fpr') rocTst_svmBing<-prediction(attributes(revBing_predTst_svm_best)$decision.values,revDTM_sentiBing_tst$hiLo)
rocTst_svmBing_auc<-performance(rocTst_svmBing,'tpr','fpr')
plot(rocTrn_svmBing_auc, col='green', legacy.axes = TRUE)
plot(rocTst_svmBing_auc, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("green", "red"), lwd=2, cex=0.8, bty='n')
abline(a = 0, b = 1)
```

```{r nrc}
#develop a SVM model on the sentiment dictionary terms
svmM1_nrc <- svm(as.factor(hiLo) ~., data = revDTM_sentinrc_trn %>%select(-review_id), kernel="radial", cost=1, scale=FALSE)
#scale is set to TRUE by default. Since all vars are in tfidf, we shud set scale=FALSE
revDTM_predTrn_svm1_nrc<-predict(svmM1_nrc, revDTM_sentinrc_trn)
table(actual= revDTM_sentinrc_trn$hiLo, predicted= revDTM_predTrn_svm1_nrc)
revDTM_predValid_svm1_nrc<-predict(svmM1_nrc, revDTM_sentinrc_valid)
table(actual= revDTM_sentinrc_valid$hiLo, predicted= revDTM_predValid_svm1_nrc)
revDTM_predTst_svm1_nrc<-predict(svmM1_nrc, revDTM_sentinrc_tst)
table(actual= revDTM_sentinrc_tst$hiLo, predicted= revDTM_predTst_svm1_nrc)
# try different parameters -- rbf kernel gamma, and cost
system.time( svmM2_nrc <- svm(as.factor(hiLo) ~., data = revDTM_sentinrc_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE))
revDTM_predTrn_svm2_nrc<-predict(svmM2_nrc, revDTM_sentinrc_trn)
table(actual= revDTM_sentinrc_trn$hiLo, predicted= revDTM_predTrn_svm2_nrc)
revDTM_predValid_svm2_nrc<-predict(svmM2_nrc, revDTM_sentinrc_valid)
table(actual= revDTM_sentinrc_valid$hiLo, predicted= revDTM_predValid_svm2_nrc)
revDTM_predTst_svm2_nrc<-predict(svmM2_nrc, revDTM_sentinrc_tst)
table(actual= revDTM_sentinrc_tst$hiLo, predicted= revDTM_predTst_svm2_nrc)
#SVM Tune code for NRC
#system.time(svm_tune_nrc <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentinrc_trn %>% select(-review_id),
#kernel="radial", ranges = list( cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10))) )
#Check performance for different tuned parameters
#svm_tune_nrc$performances
#Best model
#svm_tune_nrc$best.parameters
#svm_tune_nrc$best.model
system.time( svm_best_nrc <- svm(as.factor(hiLo) ~., data = revDTM_sentinrc_trn
%>% select(-review_id), kernel="radial", cost=10, gamma=.5, scale=FALSE,decision.values=TRUE))
#predictions from best model
revNRC_predTrn_svm_best<-predict(svm_best_nrc, revDTM_sentinrc_trn,decision.values=TRUE)
table(actual= revDTM_sentinrc_trn$hiLo, predicted= revNRC_predTrn_svm_best)
revNRC_predValid_svm_best<-predict(svm_best_nrc, revDTM_sentinrc_valid,decision.values=TRUE)
table(actual= revDTM_sentinrc_valid$hiLo, predicted= revNRC_predValid_svm_best)
revNRC_predTst_svm_best<-predict(svm_best_nrc, revDTM_sentinrc_tst,decision.values=TRUE)
table(actual= revDTM_sentinrc_tst$hiLo, predicted= revNRC_predTst_svm_best)
#ROC graph SVM COMBINED best
rocTrn_svmNRC<-prediction(attributes(revNRC_predTrn_svm_best)$decision.values,revDTM_sentinrc_trn$hiLo)
rocTst_svmNRC_auc<-performance(rocTrn_svmNRC,'tpr','fpr')
rocTst_svmNRC<-prediction(attributes(revNRC_predTst_svm_best)$decision.values,revDTM_sentinrc_tst$hiLo)
rocTst_svmNRC_auc<-performance(rocTst_svmNRC,'tpr','fpr')
plot(rocTrn_svmNRC_auc, col='green', legacy.axes = TRUE)
plot(rocTst_svmNRC_auc, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("green", "red"), lwd=2, cex=0.8, bty='n')
abline(a = 0, b = 1)
```

```{r afinn}
#develop a SVM model on the sentiment dictionary terms
svmM1_afinn <- svm(as.factor(hiLo) ~., data = revDTM_sentiafinn_trn %>%select(-review_id), kernel="radial", cost=1, scale=FALSE)
#scale is set to TRUE by default. Since all vars are in tfidf, we shud set scale=FALSE 
revDTM_predTrn_svm1_afinn<-predict(svmM1_afinn, revDTM_sentiafinn_trn)
table(actual= revDTM_sentiafinn_trn$hiLo, predicted= revDTM_predTrn_svm1_afinn)
revDTM_predValid_svm1_afinn<-predict(svmM1_afinn, revDTM_sentiafinn_valid)
table(actual= revDTM_sentiafinn_valid$hiLo, predicted= revDTM_predValid_svm1_afinn)
revDTM_predTst_svm1_afinn<-predict(svmM1_afinn, revDTM_sentiafinn_tst)
table(actual= revDTM_sentiafinn_tst$hiLo, predicted= revDTM_predTst_svm1_afinn)
# try different parameters -- rbf kernel gamma, and cost
system.time( svmM2_afinn <- svm(as.factor(hiLo) ~., data = revDTM_sentiafinn_trn%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE))
revDTM_predTrn_svm2_afinn<-predict(svmM2_afinn, revDTM_sentiafinn_trn)
table(actual= revDTM_sentiafinn_trn$hiLo, predicted= revDTM_predTrn_svm2_afinn)
revDTM_predValid_svm2_afinn<-predict(svmM2_afinn, revDTM_sentiafinn_valid)
table(actual= revDTM_sentiafinn_valid$hiLo, predicted= revDTM_predValid_svm2_afinn)
revDTM_predTst_svm2_afinn<-predict(svmM2_afinn, revDTM_sentiafinn_tst)
table(actual= revDTM_sentiafinn_tst$hiLo, predicted= revDTM_predTst_svm2_afinn)
#SVM Tune code for afinn
#system.time(svm_tune_afinn <- tune(svm, as.factor(hiLo) ~., data = revDTM_sentiafinn_trn %>% select(-review_id),
#kernel="radial", ranges = list( cost=c(0.1,1,10,50), gamma = c(0.5,1,2,5, 10))) )
#Check performance for different tuned parameters
#svm_tune_afinn$performances
#Best model
#svm_tune_afinn$best.parameters
#svm_tune_afinn$best.model
system.time( svm_best_afinn <- svm(as.factor(hiLo) ~., data = revDTM_sentiafinn_trn
%>% select(-review_id), kernel="radial", cost=10, gamma=.5, scale=FALSE,decision.values=TRUE) )
#predictions from best model
revafinn_predTrn_svm_best<-predict(svm_best_afinn, revDTM_sentiafinn_trn,decision.values=TRUE)
table(actual= revDTM_sentiafinn_trn$hiLo, predicted= revafinn_predTrn_svm_best)
revafinn_predValid_svm_best<-predict(svm_best_afinn, revDTM_sentiafinn_valid,decision.values=TRUE)
table(actual= revDTM_sentiafinn_valid$hiLo, predicted= revafinn_predValid_svm_best)
revafinn_predTst_svm_best<-predict(svm_best_afinn, revDTM_sentiafinn_tst,decision.values=TRUE)
table(actual= revDTM_sentiafinn_tst$hiLo, predicted= revafinn_predTst_svm_best)
#ROC graph SVM COMBINED best
rocTrn_svmAFINN<-prediction(attributes(revafinn_predTrn_svm_best)$decision.values,revDTM_sentiafinn_trn$hiLo)
rocTrn_svmAFINN_auc<-performance(rocTrn_svmAFINN,'tpr','fpr') rocTst_svmAFINN<-prediction(attributes(revafinn_predTst_svm_best)$decision.values,revDTM_sentiafinn_tst$hiLo)
rocTst_svmAFINN_auc<-performance(rocTst_svmAFINN,'tpr','fpr')
plot(rocTrn_svmAFINN_auc, col='green', legacy.axes = TRUE)
plot(rocTst_svmAFINN_auc, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("green", "red"), lwd=2, cex=0.8, bty='n')
abline(a = 0, b = 1)
```

```{r}
#Combining list of words from bing, nrc and affin dictionary
rrSenti_combined <- rbind(rrSenti_bing, rrSenti_nrc, rrSenti_afinn)
rrSenti_combined <- rrSenti_combined[,1:7]
rrSenti_combined <- distinct(rrSenti_combined)
#Creating Document Term Matrix
revDTM_sentiCombined <- rrSenti_combined %>% pivot_wider(id_cols = c(review_id,starsReview), names_from = word, values_from = tf_idf) %>% ungroup()
dim(revDTM_sentiCombined)
#filter out the reviews with stars=3, and calculate hiLo sentiment 'class'
revDTM_sentiCombined <- revDTM_sentiCombined %>% filter(starsReview!=3) %>% mutate(hiLo=ifelse(starsReview<=2, -1, 1)) %>% select(-starsReview)
dim(revDTM_sentiCombined)
revDTM_sentiCombined %>% group_by(hiLo) %>% tally()
#replace all the NAs with 0
revDTM_sentiCombined <- revDTM_sentiCombined %>% replace(., is.na(.), 0)
revDTM_sentiCombined$hiLo <- as.factor(revDTM_sentiCombined$hiLo)
#split the data into trn, tst subsets
set.seed(123)
nr=nrow(revDTM_sentiCombined)
trnIndex = sample(1:nr, size = round(0.5*nr), replace=FALSE)
revDTM_sentiCombined_SubSample=revDTM_sentiCombined[trnIndex,]
revDTM_sentiCombined_split<- initial_split(revDTM_sentiCombined_SubSample, 0.7) 
revDTM_sentiCombined_trn<- training(revDTM_sentiCombined_split) 
revDTM_sentiCombined_inter<- testing(revDTM_sentiCombined_split)
revDTM_sentiCombined_split_1<- initial_split(revDTM_sentiCombined_inter, 0.66)
revDTM_sentiCombined_tst<- training(revDTM_sentiCombined_split_1)
revDTM_sentiCombined_valid<- testing(revDTM_sentiCombined_split_1)
dim(revDTM_sentiCombined_trn)
dim(revDTM_sentiCombined_tst)
dim(revDTM_sentiCombined_valid)
colMeans(is.na(revDTM_sentiCombined_trn))[colMeans(is.na(revDTM_sentiCombined_trn))>0]
rm(revDTM_sentiCombined_inter)
rm(revDTM_sentiCombined_split)
rm(revDTM_sentiCombined_split_1)
rfModel_CD <-ranger(dependent.variable.name = "hiLo", data=revDTM_sentiCombined_trn %>% select(-review_id), num.trees = 200, importance='permutation', probability = TRUE)
#Obtain predictions, and calculate performance
revCD_predTrn<- predict(rfModel_CD, revDTM_sentiCombined_trn %>% select(-review_id))$predictions
revCD_predValid<- predict(rfModel_CD,revDTM_sentiCombined_valid %>% select(-review_id))$predictions
revCD_predTst<- predict(rfModel_CD, revDTM_sentiCombined_tst %>% select(-review_id))$predictions
#Confusion matrix
table(actual=revDTM_sentiCombined_trn$hiLo, preds=revCD_predTrn[,2]>0.5)
table(actual=revDTM_sentiCombined_valid$hiLo, preds=revCD_predValid[,2]>0.5)
table(actual=revDTM_sentiCombined_tst$hiLo, preds=revCD_predTst[,2]>0.5)
#ROC AUC graph for RF Combined Dictionaries
rocTrn_rfCD <- roc(revDTM_sentiCombined_trn$hiLo, revCD_predTrn[,2], levels=c(-1, 1))
rocTst_rfCD <- roc(revDTM_sentiCombined_tst$hiLo, revCD_predTst[,2], levels=c(-1, 1))
plot.roc(rocTrn_rfCD, col='blue')
plot.roc(rocTst_rfCD, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r}
nbModel_CD<-naiveBayes(hiLo ~ ., data=revDTM_sentiCombined_trn %>% select(-review_id))
revSentiCD_NBpredTrn<-predict(nbModel_CD, revDTM_sentiCombined_trn, type = "raw")
revSentiCD_NBpredTst<-predict(nbModel_CD, revDTM_sentiCombined_tst, type = "raw")
revSentiCD_NBpredValid<-predict(nbModel_CD, revDTM_sentiCombined_valid, type = "raw")
table(actual= revDTM_sentiCombined_trn$hiLo, predicted= revSentiCD_NBpredTrn[,2]>0.5)
table(actual= revDTM_sentiCombined_tst$hiLo, predicted= revSentiCD_NBpredTst[,2]>0.5)
table(actual= revDTM_sentiCombined_valid$hiLo, predicted= revSentiCD_NBpredValid[,2]>0.5)
auc(as.numeric(revDTM_sentiCombined_trn$hiLo), revSentiCD_NBpredTrn[,2])
auc(as.numeric(revDTM_sentiCombined_tst$hiLo), revSentiCD_NBpredTst[,2])
auc(as.numeric(revDTM_sentiCombined_valid$hiLo), revSentiCD_NBpredValid[,2])
rocTrn_nbCD <- roc(revDTM_sentiCombined_trn$hiLo, revSentiCD_NBpredTrn[,2], levels=c(-1, 1))
rocTst_nbCD <- roc(revDTM_sentiCombined_tst$hiLo, revSentiCD_NBpredTst[,2], levels=c(-1, 1))
plot.roc(rocTrn_nbCD, col= 'blue')
plot.roc(rocTst_nbCD, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"), col=c("blue", "red"), lwd=2, cex=0.8, bty='n')
```

```{r}
#develop a SVM model on the sentiment dictionary terms, based on tuned parameter on the previous runs.
svmM1_CD <- svm(as.factor(hiLo) ~., data = revDTM_sentiCombined_trn %>%select(-review_id), kernel="radial", cost=10, gamma=.5, scale=FALSE)
revCD_predTrn_svm1<-predict(svmM1_CD, revDTM_sentiCombined_trn,decision.values = TRUE)
table(actual= revDTM_sentiCombined_trn$hiLo, predicted= revCD_predTrn_svm1)
revCD_predValid_svm1<-predict(svmM1_CD, revDTM_sentiCombined_valid,decision.values = TRUE)
table(actual= revDTM_sentiCombined_valid$hiLo, predicted= revCD_predValid_svm1)
revCD_predTst_svm1<-predict(svmM1_CD, revDTM_sentiCombined_tst)
table(actual= revDTM_sentiCombined_tst$hiLo, predicted= revCD_predTst_svm1)
# try different parameters -- rbf kernel gamma, and cost
system.time(svmM2_CD<- svm(as.factor(hiLo) ~., data = revDTM_sentiCombined_trn
%>% select(-review_id), kernel="radial", cost=5, gamma=5, scale=FALSE,decision.values=TRUE))
revCD_predTrn_svm2<-predict(svmM2_CD, revDTM_sentiCombined_trn,decision.values = TRUE)
table(actual= revDTM_sentiCombined_trn$hiLo, predicted= revCD_predTrn_svm2)
revCD_predValid_svm2<-predict(svmM2_CD, revDTM_sentiCombined_valid,decision.values=TRUE)
table(actual= revDTM_sentiCombined_valid$hiLo, predicted= revCD_predValid_svm2)
revCD_predTst_svm2<-predict(svmM2_CD, revDTM_sentiCombined_tst,decision.values=TRUE)
table(actual= revDTM_sentiCombined_tst$hiLo, predicted= revCD_predTst_svm2)
#ROC graph SVM COMBINED best
rocTrn_svmCD<-prediction(attributes(revCD_predTrn_svm2)$decision.values,revDTM_sentiCombined_trn $hiLo)
rocTrn_svmCD_auc<-performance(rocTrn_svmCD,'tpr','fpr') 
rocTst_svmCD<-prediction(attributes(revCD_predTst_svm2)$decision.values,revDTM_sentiCombined_tst $hiLo)
rocTst_svmCD_auc<-performance(rocTst_svmCD,'tpr','fpr')
plot(rocTrn_svmCD_auc, col='green', legacy.axes = TRUE)
plot(rocTst_svmCD_auc, col='red', add=TRUE)
legend("bottomright", legend=c("Training", "Test"),col=c("green", "red"), lwd=2, cex=0.8, bty='n')
abline(a = 0, b = 1)
```

```{r}
dim(resReviewsData)
glimpse(resReviewsData)
x<- resReviewsData %>% select (review_id, attributes)
paste(x[1,2])
x2<-x %>% mutate (atts = str_split(attributes, '\\|')) %>% unnest(atts)
x3<- x2 %>% cbind( str_split_fixed ( x2$atts, ":", 2) )
colnames(x3)[4]<- 'attName'
colnames(x3)[5]<- 'attValue'
x3<-x3 %>% select (-c (attributes ,atts))
x4<- x3 %>% pivot_wider (names_from = attName,values_from = attValue)
# If there are some empty values of attName, pivot_wider can report errors,
# so filter these rows out first
x4<- x3 %>% filter(attName!="")%>% pivot_wider (names_from = attName, values_from = attValue) 
x4[1,3]
x5 <- x4 %>% mutate(amb = str_split(Ambience, ","))
extractAmbience <- function(q) {
sub(":.*","", q[which(str_extract(q, "True") == "True")])
}
x6<- x5 %>% mutate( amb = lapply(amb, extractAmbience ))
#how many examples by different values for 'Ambience'
x6 %>% group_by(amb) %>% tally() %>% view()
x6 %>% filter( str_detect (as.character(amb), 'casual')) %>% count()
x6 %>% filter( str_detect(as.character(amb), 'classy')) %>% count()
x7<- resReviewsData %>% left_join(x6)%>% select(review_id, review_count, starsReview, amb)
x7 %>% group_by(amb) %>% tally() %>% view()
x7 %>% filter(str_detect (as.character(amb), "casual"))%>% group_by(starsReview) %>% tally() 
x7 %>% filter( str_detect ( as.character(amb), "classy|trendy|upscale"))%>% group_by(starsReview) %>% tally()
x7 %>% filter( str_detect ( as.character(amb),"classy|trendy|upscale"))%>% group_by(starsReview) %>% tally()%>% mutate(prop=n/sum(n))
x7 %>% filter( str_detect ( as.character(amb), c('hipster', 'trendy')))%>% group_by(starsReview) %>% tally() %>% mutate(prop=n/sum(n))
x7 %>% filter( str_detect ( as.character(amb), "hipster|trendy"))%>% group_by(starsReview) %>% tally() %>% mutate(prop=n/sum(n))
x7 %>% group_by(starsReview) %>% summarize(nCasual= sum(str_detect (as.character(amb), "casual")))
x7 %>% group_by(starsReview) %>% summarize( nCasual= sum(str_detect (as.character(amb), "casual")),ntouristy=sum(str_detect ( as.character(amb), "touristy")) )
```
